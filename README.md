
Run LLM locally to plan, run and respond with ollama models.


### Install

```
pip install -r requirements.txt
```


### Start app

```
source venv/bin/activate
python start.py
```



#### Sample queries

```
Call nabraj.com/test.json and provide me 'message' from the response.
```

```
echo hello
```
